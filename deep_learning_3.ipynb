{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
       "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
       "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
       "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor())\n",
    "\n",
    "image_tensor, label = dataset[0]\n",
    "print(image_tensor.shape, label)\n",
    "image_tensor[:,10:15, 10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor[0,10:15,10:15], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0321,  0.0311, -0.0146,  ..., -0.0199, -0.0104, -0.0099],\n",
      "        [-0.0269,  0.0297,  0.0305,  ..., -0.0351,  0.0083, -0.0202],\n",
      "        [-0.0331,  0.0069, -0.0343,  ...,  0.0006, -0.0341,  0.0172],\n",
      "        ...,\n",
      "        [-0.0034,  0.0284,  0.0204,  ...,  0.0075, -0.0347,  0.0147],\n",
      "        [-0.0272,  0.0234, -0.0348,  ...,  0.0213, -0.0038, -0.0063],\n",
      "        [ 0.0135, -0.0275, -0.0198,  ...,  0.0071,  0.0245,  0.0220]],\n",
      "       requires_grad=True)\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0244,  0.0184,  0.0181, -0.0243,  0.0113, -0.0234, -0.0023, -0.0042,\n",
       "        -0.0187,  0.0193], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "print(model.weight)\n",
    "\n",
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 7, 0, 5, 1, 6, 0, 3, 3, 9, 2, 6, 9, 2, 2, 0, 8, 0, 3, 2, 4, 6, 9,\n",
      "        5, 4, 2, 0, 5, 3, 7, 2, 2, 0, 2, 1, 3, 2, 7, 7, 2, 7, 4, 6, 4, 1, 8, 2,\n",
      "        6, 6, 1, 3, 3, 6, 0, 1, 0, 3, 8, 9, 2, 1, 9, 8, 5, 0, 1, 0, 4, 7, 7, 2,\n",
      "        4, 1, 6, 9, 0, 7, 8, 7, 8, 6, 0, 9, 7, 5, 8, 1, 1, 4, 7, 4, 4, 4, 1, 4,\n",
      "        6, 9, 1, 4, 6, 6, 1, 8, 4, 7, 1, 2, 7, 8, 8, 1, 2, 0, 4, 3, 9, 6, 8, 6,\n",
      "        2, 8, 3, 2, 6, 6, 7, 7])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\repo\\git\\python-llm\\deep_learning_3.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repo/git/python-llm/deep_learning_3.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repo/git/python-llm/deep_learning_3.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/repo/git/python-llm/deep_learning_3.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repo/git/python-llm/deep_learning_3.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mbreak\u001b[39;00m;\n",
      "File \u001b[1;32mc:\\repo\\git\\python-llm\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repo\\git\\python-llm\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\repo\\git\\python-llm\\cuda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0214, -0.0054,  0.0059,  ..., -0.0153, -0.0311, -0.0240],\n",
       "         [-0.0147, -0.0293,  0.0068,  ..., -0.0137,  0.0165, -0.0233],\n",
       "         [ 0.0249, -0.0150, -0.0230,  ..., -0.0209,  0.0332, -0.0217],\n",
       "         ...,\n",
       "         [-0.0259,  0.0005, -0.0238,  ...,  0.0259, -0.0263,  0.0192],\n",
       "         [-0.0112, -0.0350, -0.0110,  ...,  0.0345,  0.0241, -0.0170],\n",
       "         [-0.0132, -0.0222, -0.0121,  ...,  0.0226,  0.0126,  0.0045]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0105, -0.0325,  0.0122,  0.0168, -0.0127,  0.0157,  0.0112,  0.0110,\n",
       "         -0.0321, -0.0330], requires_grad=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape:  torch.Size([128, 1, 28, 28])\n",
      "outputs.shape:  torch.Size([128, 10])\n",
      "Sample outputs:  tensor([[-0.1266, -0.0201,  0.2065, -0.2265, -0.3293,  0.0868, -0.2472, -0.0500,\n",
      "          0.2609, -0.3548],\n",
      "        [ 0.1331,  0.1236, -0.0483, -0.0363, -0.1612,  0.2361, -0.3158, -0.0208,\n",
      "          0.1153,  0.0342]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images.shape: ', images.shape)\n",
    "    outputs = model(images)\n",
    "    break;\n",
    "\n",
    "print('outputs.shape: ', outputs.shape)\n",
    "print('Sample outputs: ', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1266, -0.0201,  0.2065, -0.2265, -0.3293,  0.0868, -0.2472, -0.0500,\n",
      "         0.2609, -0.3548], grad_fn=<SelectBackward0>)\n",
      "tensor([0.8811, 0.9801, 1.2294, 0.7974, 0.7194, 1.0907, 0.7810, 0.9513, 1.2981,\n",
      "        0.7013], grad_fn=<ExpBackward0>)\n",
      "tensor([0.0934, 0.1039, 0.1304, 0.0846, 0.0763, 0.1157, 0.0828, 0.1009, 0.1377,\n",
      "        0.0744], grad_fn=<DivBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#softmax\n",
    "print(outputs[0])\n",
    "exps = torch.exp(outputs[0])\n",
    "print(exps)\n",
    "probs = exps / torch.sum(exps)\n",
    "print(probs)\n",
    "print(torch.sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001192092896"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "torch.sum(probs[0].data).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 5, 2, 7, 2, 3, 3, 8, 5, 2, 6, 8, 2, 2, 2, 8, 2, 8, 1, 1, 7, 0, 0, 7,\n",
      "        8, 3, 5, 2, 0, 0, 2, 2, 2, 5, 2, 2, 2, 2, 8, 3, 2, 8, 4, 2, 8, 6, 2, 2,\n",
      "        2, 8, 5, 7, 2, 5, 2, 8, 0, 8, 1, 0, 2, 8, 2, 5, 2, 7, 5, 0, 2, 5, 5, 5,\n",
      "        2, 2, 5, 2, 2, 8, 8, 2, 8, 2, 8, 2, 2, 8, 5, 8, 2, 5, 2, 7, 5, 6, 8, 8,\n",
      "        2, 1, 0, 8, 7, 5, 2, 2, 2, 5, 2, 8, 0, 3, 7, 1, 5, 2, 8, 3, 3, 2, 2, 2,\n",
      "        5, 8, 7, 3, 2, 3, 8, 3])\n",
      "tensor([0.1377, 0.1245, 0.1226, 0.1222, 0.1159, 0.1254, 0.1179, 0.1134, 0.1310,\n",
      "        0.1454, 0.1231, 0.1159, 0.1138, 0.1149, 0.1242, 0.1382, 0.1241, 0.1266,\n",
      "        0.1191, 0.1361, 0.1159, 0.1330, 0.1235, 0.1263, 0.1463, 0.1197, 0.1229,\n",
      "        0.1274, 0.1310, 0.1406, 0.1356, 0.1369, 0.1187, 0.1181, 0.1389, 0.1257,\n",
      "        0.1364, 0.1177, 0.1177, 0.1277, 0.1148, 0.1361, 0.1155, 0.1260, 0.1156,\n",
      "        0.1212, 0.1425, 0.1510, 0.1301, 0.1221, 0.1199, 0.1296, 0.1591, 0.1366,\n",
      "        0.1907, 0.1209, 0.1307, 0.1393, 0.1397, 0.1440, 0.1951, 0.1328, 0.1447,\n",
      "        0.1243, 0.1585, 0.1173, 0.1289, 0.1305, 0.1339, 0.1375, 0.1326, 0.1317,\n",
      "        0.1237, 0.1119, 0.1355, 0.1547, 0.1335, 0.1292, 0.1246, 0.1530, 0.1456,\n",
      "        0.1671, 0.1494, 0.1500, 0.1422, 0.1269, 0.1212, 0.1267, 0.1147, 0.1117,\n",
      "        0.1237, 0.1279, 0.1334, 0.1235, 0.1511, 0.1420, 0.1426, 0.1265, 0.1433,\n",
      "        0.1276, 0.1229, 0.1251, 0.1459, 0.1126, 0.1229, 0.1269, 0.1234, 0.1337,\n",
      "        0.1329, 0.1191, 0.1360, 0.1264, 0.1203, 0.1559, 0.1301, 0.1175, 0.1211,\n",
      "        0.1233, 0.1345, 0.1426, 0.1283, 0.1371, 0.1367, 0.1302, 0.1164, 0.1252,\n",
      "        0.1511, 0.1222], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 1, 6, 2, 4, 0, 3, 3, 3, 8, 6, 6, 4, 4, 9, 3, 9, 7, 8, 4, 5, 3, 2, 5,\n",
       "        6, 2, 1, 2, 3, 0, 6, 3, 2, 1, 0, 2, 6, 1, 7, 7, 8, 0, 5, 5, 8, 8, 7, 9,\n",
       "        5, 0, 1, 9, 4, 1, 8, 9, 3, 1, 8, 0, 5, 9, 8, 1, 3, 8, 1, 6, 2, 6, 2, 1,\n",
       "        3, 3, 1, 9, 1, 9, 2, 4, 9, 6, 6, 9, 3, 8, 1, 9, 8, 3, 9, 8, 2, 8, 8, 6,\n",
       "        4, 8, 0, 2, 7, 1, 6, 1, 0, 8, 6, 5, 7, 1, 7, 8, 3, 4, 3, 9, 1, 7, 2, 7,\n",
       "        1, 2, 7, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2839, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3137121200561523, 'val_acc': 0.08910205960273743}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9429, val_acc: 0.6107\n",
      "Epoch [1], val_loss: 1.6739, val_acc: 0.7214\n",
      "Epoch [2], val_loss: 1.4730, val_acc: 0.7599\n",
      "Epoch [3], val_loss: 1.3212, val_acc: 0.7809\n",
      "Epoch [4], val_loss: 1.2045, val_acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.9791, val_acc: 0.8236\n",
      "Epoch [1], val_loss: 0.8500, val_acc: 0.8354\n",
      "Epoch [2], val_loss: 0.7667, val_acc: 0.8436\n",
      "Epoch [3], val_loss: 0.7078, val_acc: 0.8495\n",
      "Epoch [4], val_loss: 0.6640, val_acc: 0.8539\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.003, model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
